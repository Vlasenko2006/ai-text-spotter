"""
Jury detector using Groq Llama 3.1 8B as final arbitrator.
Makes final classification decisions based on other detector outputs.
"""
import json
import logging
from typing import Dict, Optional
import httpx
from app.config import settings

logger = logging.getLogger(__name__)


class JuryDetector:
    """
    Jury detector that uses Groq Llama 3.1 8B to make final classification decisions.
    
    Receives input from mathematical and LLM detectors and makes a final judgment.
    """
    
    def __init__(self):
        """Initialize the jury detector."""
        self.api_key = settings.groq_api_key
        self.model = settings.groq_model
        self.api_url = "https://api.groq.com/openai/v1/chat/completions"
        self._available = bool(self.api_key)
    
    def decide(
        self,
        sentence: str,
        context: Dict[str, Optional[str]],
        math_result: dict,
        llm_result: dict,
        ai_pattern_score: float = 0.5,
        predictability_score: float = 0.5
    ) -> dict:
        """
        Make final classification decision using Groq Llama.
        
        Args:
            sentence: The sentence to classify
            context: Dict with 'before' and 'after' context sentences
            math_result: Results from mathematical detector
            llm_result: Results from LLM detector
            
        Returns:
            dict with 'classification' (human/suspicious/ai), 'confidence', 'reasoning'
        """
        if not self._available:
            # Fallback to simple voting if API key not available
            return self._fallback_decision(math_result, llm_result)
        
        try:
            # Build the prompt
            prompt = self._build_prompt(sentence, context, math_result, llm_result,
                                       ai_pattern_score, predictability_score)
            
            # Call Groq API
            response = self._call_groq_api(prompt)
            
            # Parse response
            result = self._parse_response(response)
            
            return result
            
        except Exception as e:
            logger.error(f"Error in jury decision: {e}")
            # Fallback to simple voting
            return self._fallback_decision(math_result, llm_result, ai_pattern_score, predictability_score)
    
    def _build_prompt(
        self,
        sentence: str,
        context: Dict[str, Optional[str]],
        math_result: dict,
        llm_result: dict,
        ai_pattern_score: float,
        predictability_score: float
    ) -> str:
        """Build the prompt for Groq API."""
        before = context.get('before', '')
        after = context.get('after', '')
        
        math_score = math_result.get('score', 0.5)
        math_features = math_result.get('features', {})
        llm_score = llm_result.get('score', 0.5)
        llm_confidence = llm_result.get('confidence', 0.0)
        
        prompt = f"""You are an AI text detection arbitrator. Analyze this sentence from a cover letter.

Sentence: "{sentence}"
Context Before: "{before}"
Context After: "{after}"

Detector Results:
- Statistical Analysis: {math_score:.3f} (0=AI, 1=Human)
  Features: burstiness={math_features.get('burstiness', 0):.3f}, vocabulary_richness={math_features.get('vocabulary_richness', 0):.3f}, word_frequency={math_features.get('word_frequency', 0):.3f}, punctuation={math_features.get('punctuation', 0):.3f}, complexity={math_features.get('complexity', 0):.3f}, entropy={math_features.get('entropy', 0):.3f}
- Language Model Analysis: {llm_score:.3f} (0=AI, 1=Human, confidence: {llm_confidence:.3f})
- AI Pattern Analysis: {ai_pattern_score:.3f} (0=AI-wordy/repetitive, 1=Human-specific/varied)
- Predictability Analysis: {predictability_score:.3f} (0=AI-predictable, 1=Human-unique phrasing)

ðŸ” NEW POWERFUL DETECTORS:
- AI Pattern: Tests for wordy language, buzzwords, generic phrases, lack of contractions
- Predictability: Uses phrase masking - can AI predict removed words? (High prediction = AI)

Classify this sentence as: human, suspicious, or ai

Guidelines:
- "human": If Statistical > 0.5 OR AI Pattern > 0.5 OR Predictability > 0.5 - prefer human when uncertain
- "suspicious": When detectors disagree significantly
- "ai": When MULTIPLE detectors show AI patterns (Statistical < 0.4 AND (AI Pattern < 0.4 OR Predictability < 0.4))
- AI text is: wordy, uses buzzwords, predictable phrases, no contractions, vague generalities
- Human text: specific details, varied sentences, natural contractions, unique phrasing
- When in doubt, classify as human or suspicious

Respond ONLY with JSON in this exact format:
{{"classification": "human|suspicious|ai", "confidence": 0.0-1.0, "reasoning": "brief explanation"}}"""
        
        return prompt
    
    def _call_groq_api(self, prompt: str) -> str:
        """Call Groq API with the prompt."""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": self.model,
            "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "temperature": 0.1,  # Low temperature for consistent results
            "max_tokens": 200
        }
        
        with httpx.Client(timeout=30.0) as client:
            response = client.post(self.api_url, headers=headers, json=data)
            response.raise_for_status()
            
            result = response.json()
            content = result['choices'][0]['message']['content']
            
            return content
    
    def _parse_response(self, response: str) -> dict:
        """Parse the JSON response from Groq API."""
        try:
            # Extract JSON from response (might have markdown code blocks)
            response = response.strip()
            if response.startswith('```json'):
                response = response[7:]
            if response.startswith('```'):
                response = response[3:]
            if response.endswith('```'):
                response = response[:-3]
            response = response.strip()
            
            # Parse JSON
            data = json.loads(response)
            
            classification = data.get('classification', 'suspicious').lower()
            confidence = float(data.get('confidence', 0.5))
            reasoning = data.get('reasoning', 'No reasoning provided')
            
            # Validate classification
            if classification not in ['human', 'suspicious', 'ai']:
                classification = 'suspicious'
            
            # Validate confidence
            confidence = max(0.0, min(1.0, confidence))
            
            return {
                'classification': classification,
                'confidence': round(confidence, 4),
                'reasoning': reasoning
            }
            
        except Exception as e:
            logger.error(f"Error parsing Groq response: {e}")
            logger.debug(f"Response was: {response}")
            return {
                'classification': 'suspicious',
                'confidence': 0.5,
                'reasoning': 'Failed to parse response'
            }
    
    def _fallback_decision(
        self,
        math_result: dict,
        llm_result: dict,
        ai_pattern_score: float,
        predictability_score: float
    ) -> dict:
        """
        Fallback decision logic when Groq API is not available.
        Uses weighted combination of all 4 detectors.
        """
        math_score = math_result.get('score', 0.5)
        llm_score = llm_result.get('score', 0.5)
        
        logger.info(f"Jury decision - Math: {math_score:.4f}, LLM: {llm_score:.4f}, "
                   f"AI Pattern: {ai_pattern_score:.4f}, Predictability: {predictability_score:.4f}")
        
        # Weighted ensemble: All 4 detectors
        # Weights: AI Pattern & Predictability are most reliable (target wordy/predictable patterns)
        weighted_score = (
            math_score * 0.20 +
            llm_score * 0.15 +
            ai_pattern_score * 0.35 +      # Strong weight: tests wordiness/patterns
            predictability_score * 0.30     # Strong weight: tests phrase uniqueness
        )
        
        reasoning = f'Ensemble: Math={math_score:.2f}, LLM={llm_score:.2f}, AI Pattern={ai_pattern_score:.2f}, Predictability={predictability_score:.2f}'
        
        logger.info(f"Weighted ensemble score: {weighted_score:.4f}")
        elif llm_score > 0.60:
            # LLM strongly indicates human - trust it more
            weighted_score = (math_score * 0.60) + (llm_score * 0.40)
            reasoning = 'LLM shows strong human indicators - weighted 60% math, 40% LLM'
            logger.info(f"LLM shows strong human signal ({llm_score:.4f}), weighted: {weighted_score:.4f}")
        else:
            # LLM in uncertain range - mostly trust math
            weighted_score = (math_score * 0.85) + (llm_score * 0.15)
            reasoning = 'Weighted analysis (85% statistical, 15% LLM)'
            logger.info(f"Mixed signals, weighted: {weighted_score:.4f}")
        
        # Balanced thresholds
        if weighted_score > 0.50:  # Raised to be more selective
            classification = 'human'
            confidence = weighted_score
        elif weighted_score < 0.35:  # Raised to catch more AI
            classification = 'ai'
            confidence = 1.0 - weighted_score
            reasoning = 'Strong AI indicators'
        else:
            classification = 'suspicious'
            confidence = 0.5
            reasoning = 'Borderline - manual review recommended'
        
        logger.info(f"Final classification: {classification} (confidence: {confidence:.4f})")
        
        return {
            'classification': classification,
            'confidence': round(confidence, 4),
            'reasoning': reasoning
        }
    
    def is_available(self) -> bool:
        """Check if the jury detector is available (API key configured)."""
        return self._available
