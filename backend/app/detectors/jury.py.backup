"""
Jury detector using Groq Llama 3.1 8B as final arbitrator.
Makes final classification decisions based on other detector outputs.
"""
import json
import logging
from typing import Dict, Optional
import httpx
from app.config import settings

logger = logging.getLogger(__name__)


class JuryDetector:
    """
    Jury detector that uses Groq Llama 3.1 8B to make final classification decisions.
    
    Receives input from mathematical and LLM detectors and makes a final judgment.
    """
    
    def __init__(self):
        """Initialize the jury detector."""
        self.api_key = settings.groq_api_key
        self.model = settings.groq_model
        self.api_url = "https://api.groq.com/openai/v1/chat/completions"
        self._available = bool(self.api_key)
    
    def decide(
        self,
        sentence: str,
        context: Dict[str, Optional[str]],
        math_result: dict,
        llm_result: dict
    ) -> dict:
        """
        Make final classification decision using Groq Llama.
        
        Args:
            sentence: The sentence to classify
            context: Dict with 'before' and 'after' context sentences
            math_result: Results from mathematical detector
            llm_result: Results from LLM detector
            
        Returns:
            dict with 'classification' (human/suspicious/ai), 'confidence', 'reasoning'
        """
        if not self._available:
            # Fallback to simple voting if API key not available
            return self._fallback_decision(math_result, llm_result)
        
        try:
            # Build the prompt
            prompt = self._build_prompt(sentence, context, math_result, llm_result)
            
            # Call Groq API
            response = self._call_groq_api(prompt)
            
            # Parse response
            result = self._parse_response(response)
            
            return result
            
        except Exception as e:
            logger.error(f"Error in jury decision: {e}")
            # Fallback to simple voting
            return self._fallback_decision(math_result, llm_result)
    
    def _build_prompt(
        self,
        sentence: str,
        context: Dict[str, Optional[str]],
        math_result: dict,
        llm_result: dict
    ) -> str:
        """Build the prompt for Groq API."""
        before = context.get('before', '')
        after = context.get('after', '')
        
        math_score = math_result.get('score', 0.5)
        math_features = math_result.get('features', {})
        llm_score = llm_result.get('score', 0.5)
        llm_confidence = llm_result.get('confidence', 0.0)
        
        prompt = f"""You are an AI text detection arbitrator. Analyze this sentence from a cover letter.

Sentence: "{sentence}"
Context Before: "{before}"
Context After: "{after}"

Detector Results:
- Statistical Analysis: {math_score:.3f} (0=AI, 1=Human)
  Features: burstiness={math_features.get('burstiness', 0):.3f}, vocabulary_richness={math_features.get('vocabulary_richness', 0):.3f}, word_frequency={math_features.get('word_frequency', 0):.3f}, punctuation={math_features.get('punctuation', 0):.3f}, complexity={math_features.get('complexity', 0):.3f}, entropy={math_features.get('entropy', 0):.3f}
- Language Model Analysis: {llm_score:.3f} (0=AI, 1=Human, confidence: {llm_confidence:.3f})

Classify this sentence as: human, suspicious, or ai

Guidelines:
- "human": If Statistical Analysis > 0.5 OR either detector > 0.4 - default to human when uncertain
- "suspicious": Only when detectors strongly disagree (one <0.3, other >0.4)
- "ai": ONLY when Statistical Analysis < 0.4 AND shows clear AI patterns
- CRITICAL WARNING: The LLM detector is UNRELIABLE and over-aggressive!
  * If LLM score < 0.3, IGNORE IT COMPLETELY - it's almost certainly wrong
  * The Statistical Analysis is MORE TRUSTWORTHY than the LLM detector
  * LLM gives false positives on professional human text with errors
- Grammatical errors, typos, awkward phrasing = HUMAN (AI doesn't make these mistakes)
- Professional cover letters, formal writing, and polished text are almost always HUMAN
- When in doubt, classify as human or suspicious, NEVER ai unless Statistical Analysis < 0.4

Respond ONLY with JSON in this exact format:
{{"classification": "human|suspicious|ai", "confidence": 0.0-1.0, "reasoning": "brief explanation"}}"""
        
        return prompt
    
    def _call_groq_api(self, prompt: str) -> str:
        """Call Groq API with the prompt."""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": self.model,
            "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "temperature": 0.1,  # Low temperature for consistent results
            "max_tokens": 200
        }
        
        with httpx.Client(timeout=30.0) as client:
            response = client.post(self.api_url, headers=headers, json=data)
            response.raise_for_status()
            
            result = response.json()
            content = result['choices'][0]['message']['content']
            
            return content
    
    def _parse_response(self, response: str) -> dict:
        """Parse the JSON response from Groq API."""
        try:
            # Extract JSON from response (might have markdown code blocks)
            response = response.strip()
            if response.startswith('```json'):
                response = response[7:]
            if response.startswith('```'):
                response = response[3:]
            if response.endswith('```'):
                response = response[:-3]
            response = response.strip()
            
            # Parse JSON
            data = json.loads(response)
            
            classification = data.get('classification', 'suspicious').lower()
            confidence = float(data.get('confidence', 0.5))
            reasoning = data.get('reasoning', 'No reasoning provided')
            
            # Validate classification
            if classification not in ['human', 'suspicious', 'ai']:
                classification = 'suspicious'
            
            # Validate confidence
            confidence = max(0.0, min(1.0, confidence))
            
            return {
                'classification': classification,
                'confidence': round(confidence, 4),
                'reasoning': reasoning
            }
            
        except Exception as e:
            logger.error(f"Error parsing Groq response: {e}")
            logger.debug(f"Response was: {response}")
            return {
                'classification': 'suspicious',
                'confidence': 0.5,
                'reasoning': 'Failed to parse response'
            }
    
    def _fallback_decision(self, math_result: dict, llm_result: dict) -> dict:
        """
        Fallback decision logic when Groq API is not available.
        Heavily weights mathematical detector over unreliable LLM detector.
        """
        math_score = math_result.get('score', 0.5)
        llm_score = llm_result.get('score', 0.5)
        
        logger.info(f"Jury decision - Math: {math_score:.4f}, LLM: {llm_score:.4f}")
        
        # New strategy: Low LLM scores are VALUABLE evidence of AI
        # High math + Low LLM = Conflicting signals (use weighted)
        # Low math + Low LLM = Strong AI indicators
        
        if llm_score < 0.25:
            # LLM strongly indicates AI - give it MORE weight, not less!
            weighted_score = (math_score * 0.60) + (llm_score * 0.40)
            reasoning = 'LLM shows strong AI indicators - weighted 60% math, 40% LLM'
            logger.info(f"LLM shows strong AI signal ({llm_score:.4f}), weighted: {weighted_score:.4f}")
        elif llm_score > 0.60:
            # LLM strongly indicates human - trust it more
            weighted_score = (math_score * 0.60) + (llm_score * 0.40)
            reasoning = 'LLM shows strong human indicators - weighted 60% math, 40% LLM'
            logger.info(f"LLM shows strong human signal ({llm_score:.4f}), weighted: {weighted_score:.4f}")
        else:
            # LLM in uncertain range - mostly trust math
            weighted_score = (math_score * 0.85) + (llm_score * 0.15)
            reasoning = 'Weighted analysis (85% statistical, 15% LLM)'
            logger.info(f"Mixed signals, weighted: {weighted_score:.4f}")
        
        # Balanced thresholds
        if weighted_score > 0.50:  # Raised to be more selective
            classification = 'human'
            confidence = weighted_score
        elif weighted_score < 0.35:  # Raised to catch more AI
            classification = 'ai'
            confidence = 1.0 - weighted_score
            reasoning = 'Strong AI indicators'
        else:
            classification = 'suspicious'
            confidence = 0.5
            reasoning = 'Borderline - manual review recommended'
        
        logger.info(f"Final classification: {classification} (confidence: {confidence:.4f})")
        
        return {
            'classification': classification,
            'confidence': round(confidence, 4),
            'reasoning': reasoning
        }
    
    def is_available(self) -> bool:
        """Check if the jury detector is available (API key configured)."""
        return self._available
